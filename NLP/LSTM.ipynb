{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01997cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load IMDb dataset from HuggingFace\n",
    "dataset = load_dataset(\"imdb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "effa9c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc494f6b4f340e3ab47a9fb146ae8a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shailymishra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shailymishra\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a36fd555f60485c82b067bcf80cd735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70355eed6eef42dfa62cc5ab83f958a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128e1cee899b40e9bea651eaa78bb41e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d6ee067b02f4c3d81ccacdbfb9db724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f32f83153e094f50877719e06f681b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d89005585fc4481807668970b4a0957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # we just use the tokenizer, not BERT\n",
    "\n",
    "MAX_LEN = 200\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "encoded_dataset = dataset.map(tokenize, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bea8819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample review text:\n",
      "I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n",
      "\n",
      "Tokenized input_ids shape: torch.Size([1, 200])\n",
      "First 10 token IDs: tensor([  101,  1045, 12524,  1045,  2572,  8025,  1011,  3756,  2013,  2026])\n"
     ]
    }
   ],
   "source": [
    "# Show a sample input review\n",
    "sample_text = dataset[\"train\"][0][\"text\"]\n",
    "print(\"Sample review text:\")\n",
    "print(sample_text)\n",
    "\n",
    "# Tokenize the sample text\n",
    "sample_tokens = tokenizer(sample_text, padding=\"max_length\", truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "print(\"\\nTokenized input_ids shape:\", sample_tokens[\"input_ids\"].shape)\n",
    "print(\"First 10 token IDs:\", sample_tokens[\"input_ids\"][0][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63423bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [101, 2023, 3185, 2001, 1037, 2561, 5949, 1997, 2051, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Tokens:    ['[CLS]', 'this', 'movie', 'was', 'a', 'total', 'waste', 'of', 'time', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "# Use the tokenizer to encode text\n",
    "encoded = tokenizer(\"This movie was a total waste of time.\", \n",
    "                    padding=\"max_length\", truncation=True, max_length=20, return_tensors=\"pt\")\n",
    "\n",
    "# Extract input IDs\n",
    "input_ids = encoded[\"input_ids\"][0]\n",
    "\n",
    "# Convert to tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "print(\"Token IDs:\", input_ids.tolist())\n",
    "print(\"Tokens:   \", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8c5dd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded back to text: this movie was a total waste of time.\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "print(\"Decoded back to text:\", decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b822bc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = {\n",
    "    \"input_ids\": encoded_dataset[\"train\"][\"input_ids\"],\n",
    "    \"attention_mask\": encoded_dataset[\"train\"][\"attention_mask\"]\n",
    "}\n",
    "train_labels = encoded_dataset[\"train\"][\"label\"]\n",
    "\n",
    "test_encodings = {\n",
    "    \"input_ids\": encoded_dataset[\"test\"][\"input_ids\"],\n",
    "    \"attention_mask\": encoded_dataset[\"test\"][\"attention_mask\"]\n",
    "}\n",
    "test_labels = encoded_dataset[\"test\"][\"label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdbdf347",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "609bd6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "test_dataset = IMDbDataset(test_encodings, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef8d0ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(LSTMSentiment, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "        out = self.fc(hidden[-1])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "caf96adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6940\n",
      "Epoch 2, Loss: 0.6903\n",
      "Epoch 3, Loss: 0.5630\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "model = LSTMSentiment(VOCAB_SIZE, 128, 256, 2).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ae86985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7946\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aaff1bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample predictions:\n",
      "\n",
      "Review: I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Bab...\n",
      "True label: negative\n",
      "Predicted label: positive\n",
      "\n",
      "Review: Worth the entertainment value of a rental, especially if you like action movies. This one features the usual car chases, fights with the great Van Damme kick style, shooting battles with the 40 shell ...\n",
      "True label: negative\n",
      "Predicted label: negative\n",
      "\n",
      "Review: its a totally average film with a few semi-alright action sequences that make the plot seem a little better and remind the viewer of the classic van dam films. parts of the plot don't make sense and s...\n",
      "True label: negative\n",
      "Predicted label: negative\n",
      "\n",
      "Review: STAR RATING: ***** Saturday Night **** Friday Night *** Friday Morning ** Sunday Night * Monday Morning <br /><br />Former New Orleans homicide cop Jack Robideaux (Jean Claude Van Damme) is re-assigne...\n",
      "True label: negative\n",
      "Predicted label: positive\n",
      "\n",
      "Review: First off let me say, If you haven't enjoyed a Van Damme movie since bloodsport, you probably will not like this movie. Most of these movies may not have the best plots or best actors but I enjoy thes...\n",
      "True label: negative\n",
      "Predicted label: positive\n",
      "\n",
      "Review: I had high hopes for this one until they changed the name to 'The Shepherd : Border Patrol, the lamest movie name ever, what was wrong with just 'The Shepherd'. This is a by the numbers action flick t...\n",
      "True label: negative\n",
      "Predicted label: negative\n",
      "\n",
      "Review: Isaac Florentine has made some of the best western Martial Arts action movies ever produced. In particular US Seals 2, Cold Harvest, Special Forces and Undisputed 2 are all action classics. You can te...\n",
      "True label: negative\n",
      "Predicted label: positive\n",
      "\n",
      "Review: It actually pains me to say it, but this movie was horrible on every level. The blame does not lie entirely with Van Damme as you can see he tried his best, but let's face it, he's almost fifty, how m...\n",
      "True label: negative\n",
      "Predicted label: negative\n",
      "\n",
      "Review: Technically I'am a Van Damme Fan, or I was. this movie is so bad that I hated myself for wasting those 90 minutes. Do not let the name Isaac Florentine (Undisputed II) fool you, I had big hopes for th...\n",
      "True label: negative\n",
      "Predicted label: negative\n",
      "\n",
      "Review: Honestly awful film, bad editing, awful lighting, dire dialog and scrappy screenplay.<br /><br />The lighting at is so bad there's moments you can't even see what's going on, I even tried to playing w...\n",
      "True label: negative\n",
      "Predicted label: negative\n",
      "\n",
      "Review: This flick is a waste of time.I expect from an action movie to have more than 2 explosions and some shooting.Van Damme's acting is awful. He never was much of an actor, but here it is worse.He was def...\n",
      "True label: negative\n",
      "Predicted label: negative\n",
      "\n",
      "Review: Blind Date (Columbia Pictures, 1934), was a decent film, but I have a few issues with this film. First of all, I don't fault the actors in this film at all, but more or less, I have a problem with the...\n",
      "True label: negative\n",
      "Predicted label: negative\n",
      "\n",
      "Review: I first watched this movie back in the mid/late 80's, when I was a kid. We couldn't even get all the way through it. The dialog, the acting, everything about it was just beyond lame.<br /><br />Here a...\n",
      "True label: negative\n",
      "Predicted label: negative\n",
      "\n",
      "Review: I saw the Mogul Video VHS of this. That's another one of those old 1980s distributors whose catalog I wish I had!<br /><br />This movie was pretty poor. Though retitled \"Don't Look in the Attic,\" the ...\n",
      "True label: negative\n",
      "Predicted label: negative\n",
      "\n",
      "Review: A group of heirs to a mysterious old mansion find out that they have to live in it as part of a clause in the will or be disinherited, but they soon find out of its history of everybody whom had lived...\n",
      "True label: negative\n",
      "Predicted label: negative\n"
     ]
    }
   ],
   "source": [
    "# Show predictions on 5 test samples\n",
    "print(\"\\nSample predictions:\")\n",
    "label_map = {0: \"negative\", 1: \"positive\"}\n",
    "\n",
    "for i in range(15):\n",
    "    text = dataset[\"test\"][i][\"text\"]\n",
    "    true_label = dataset[\"test\"][i][\"label\"]\n",
    "\n",
    "    # Tokenize single example\n",
    "    inputs = tokenizer(text, padding=\"max_length\", truncation=True, max_length=MAX_LEN, return_tensors=\"pt\").to(device)\n",
    "    outputs = model(inputs[\"input_ids\"])\n",
    "    pred = torch.argmax(outputs, dim=1).item()\n",
    "\n",
    "    print(f\"\\nReview: {text[:200]}...\")\n",
    "    print(f\"True label: {label_map[true_label]}\")\n",
    "    print(f\"Predicted label: {label_map[pred]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f7fd49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
