{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a small corpus\n",
    "corpus = [\n",
    "    \"travel is good for life\",\n",
    "    \"Machine learning is great\",\n",
    "    \"I love coding in Python\",\n",
    "    \"Python is great for data science\",\n",
    "    \"Data science and machine learning\"\n",
    "]\n",
    "\n",
    "# Preprocessing: Tokenizing and building a vocabulary\n",
    "def preprocess(corpus):\n",
    "    sentences = [sentence.lower().split() for sentence in corpus]\n",
    "    vocabulary = sorted(set(word for sentence in sentences for word in sentence))  # Sorted for consistency\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "    return sentences, word2idx, idx2word\n",
    "\n",
    "sentences, word2idx, idx2word = preprocess(corpus)\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "# Function to generate training data (Skip-Gram pairs)\n",
    "def generate_training_data(sentences, word2idx, window_size=2):\n",
    "    training_pairs = []\n",
    "    for sentence in sentences:\n",
    "        indices = [word2idx[word] for word in sentence]\n",
    "        for center_pos, center_word in enumerate(indices):\n",
    "            # Define context window range\n",
    "            start = max(center_pos - window_size, 0)\n",
    "            end = min(center_pos + window_size + 1, len(indices))\n",
    "            \n",
    "            for context_pos in range(start, end):\n",
    "                if context_pos != center_pos:\n",
    "                    training_pairs.append((center_word, indices[context_pos]))  # (center, context)\n",
    "    \n",
    "    return training_pairs\n",
    "\n",
    "# Generate training data\n",
    "window_size = 2\n",
    "training_data = generate_training_data(sentences, word2idx, window_size)\n",
    "\n",
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 194.0781\n",
      "Epoch 500, Loss: 139.3015\n",
      "Final Word Embeddings:\n",
      " [[-3.70250586  3.04311431 -2.87079577  0.93378752 -3.37710457  0.30585486\n",
      "  -7.88808373 -3.40512662  9.34577285 -0.04867573]\n",
      " [-1.94157789  4.08769833 -3.1746407   1.27255238 -7.16394152 -0.03277311\n",
      "  -8.19499915 -1.41634568  7.88671122 -0.35021732]\n",
      " [-2.37052118  3.86808067 -2.02327506  0.83384659 -4.98637613  0.90389342\n",
      "  -8.15112657 -2.80040922  8.62704307 -0.63626952]\n",
      " [-3.549503    4.81954789 -2.52086693  0.59116087 -5.80785836  0.28978125\n",
      "  -7.9971631  -2.88677258  9.42812436 -0.40793367]\n",
      " [-3.1768224   4.67606979 -1.91007481  0.44906666 -5.83989496  0.62080776\n",
      "  -7.58267682 -3.0241386   9.58930094 -0.35760592]\n",
      " [-3.21980031  4.64678263 -2.08728947  0.49263938 -5.87115679  0.57630485\n",
      "  -7.73250517 -2.46385474  9.88980851 -0.58735502]\n",
      " [-0.61691286  3.41310629 -3.7676008   0.79689075 -8.11717915  1.14704039\n",
      "  -8.45846825  0.20033015  8.54508355  0.77397014]\n",
      " [-0.71802307  4.73108789 -3.07392455  1.5254088  -7.32685357  1.08145257\n",
      "  -7.84784029 -0.84826167  8.20582374 -0.23282867]\n",
      " [-3.09167233  4.44644003 -1.92121417  0.75917271 -6.1223915   0.36745898\n",
      "  -7.83713446 -2.42153871  9.8327855  -0.15664002]\n",
      " [-3.73169335  4.58934946 -2.70801968  0.48524035 -5.45466322  0.5610253\n",
      "  -7.65743492 -3.11286135  9.66384578 -0.18376358]\n",
      " [-2.96016824  2.9061757  -1.57873814  1.26808438 -4.92439955  0.64119686\n",
      "  -8.03420697 -4.50325584  7.05198837 -0.08666779]\n",
      " [-1.1016421   3.46156866 -3.46672446  1.26615212 -8.21441172  0.24801087\n",
      "  -7.61780771 -1.36393687  8.19507965  0.40064415]\n",
      " [-3.35737566  4.33072008 -2.53767444  0.39391097 -5.47360357  0.66533897\n",
      "  -7.19899059 -2.9218697   9.58538932 -0.06671137]\n",
      " [-2.73693882  4.96656338 -2.28713926  1.59978521 -6.41510555  0.4018896\n",
      "  -7.87103    -1.39375116  9.29224968 -0.93981098]\n",
      " [-2.60028384  3.26622781 -2.41865687  0.51921885 -4.89077827  0.85113525\n",
      "  -7.99534515 -3.26890458  9.21583174 -0.44137153]\n",
      " [-4.60285663  5.69603328 -1.57628911  2.41792975 -5.97809697 -0.34842461\n",
      "  -6.44253182 -3.83175656  8.96175079 -0.33021735]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleSkipGram:\n",
    "    def __init__(self, vocab_size, embedding_dim, learning_rate=0.05):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Randomly initialize embeddings\n",
    "        self.U = np.random.randn(vocab_size, embedding_dim) * 0.01  # Context matrix\n",
    "        self.V = np.random.randn(vocab_size, embedding_dim) * 0.01  # Center matrix\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"Compute softmax row-wise with numerical stability.\"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Stability trick\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    def compute_loss_and_gradients(self, center_word, context_word):\n",
    "        \"\"\"\n",
    "        Compute gradients for a single (center, context) word pair.\n",
    "        center_word: index of center word\n",
    "        context_word: index of context word\n",
    "        \"\"\"\n",
    "        # Compute scores (dot product between all word pairs)\n",
    "        scores = self.U @ self.V.T  # Shape: (vocab_size, vocab_size)\n",
    "        P = self.softmax(scores)  # Softmax probabilities\n",
    "\n",
    "        # Compute loss\n",
    "        loss = -np.log(P[context_word, center_word])  # Negative log-likelihood\n",
    "\n",
    "        # Compute gradients\n",
    "        dV = np.zeros_like(self.V)\n",
    "        dU = np.zeros_like(self.U)\n",
    "\n",
    "        # Update center word vector gradient\n",
    "        dV[center_word] = -self.U[context_word] + np.sum(P[:, center_word, np.newaxis] * self.U, axis=0)\n",
    "        \n",
    "        # Update context word vector gradient\n",
    "        dU[context_word] = -self.V[center_word] + np.sum(P[context_word, :, np.newaxis] * self.V, axis=0)\n",
    "\n",
    "        return loss, dU, dV\n",
    "\n",
    "    def train(self, dataset, epochs=500):\n",
    "        \"\"\"\n",
    "        Train model on word pairs (center, context).\n",
    "        dataset: List of (center_word_index, context_word_index) pairs.\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            dU_total = np.zeros_like(self.U)\n",
    "            dV_total = np.zeros_like(self.V)\n",
    "\n",
    "            for center, context in dataset:\n",
    "                loss, dU, dV = self.compute_loss_and_gradients(center, context)\n",
    "                total_loss += loss\n",
    "                dU_total += dU\n",
    "                dV_total += dV\n",
    "\n",
    "            # Gradient descent updates\n",
    "            self.U -= self.learning_rate * dU_total\n",
    "            self.V -= self.learning_rate * dV_total\n",
    "\n",
    "            if epoch % 500 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "    def get_word_vectors(self):\n",
    "        \"\"\"Return learned word embeddings (average of U and V).\"\"\"\n",
    "        return (self.U + self.V) / 2\n",
    "\n",
    "\n",
    "embedding_dim = 10\n",
    "# Initialize and train model\n",
    "model = SimpleSkipGram(vocab_size, embedding_dim, learning_rate=0.1)\n",
    "model.train(training_data, epochs=1000)\n",
    "\n",
    "# Get trained word embeddings\n",
    "word_vectors = model.get_word_vectors()\n",
    "print(\"Final Word Embeddings:\\n\", word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Embedding Similarity Results:\n",
      "Similarity between 'machine' and 'python': 0.9994\n",
      "Similarity between 'travel' and 'learning': 0.9691\n",
      "Word embedding for 'data': [-2.37052118  3.86808067 -2.02327506  0.83384659 -4.98637613  0.90389342\n",
      " -8.15112657 -2.80040922  8.62704307 -0.63626952]\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(word):\n",
    "    \"\"\"\n",
    "    Retrieves the learned embedding for a given word.\n",
    "    \"\"\"\n",
    "    return word_vectors[word2idx[word]]\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two word vectors.\n",
    "    \"\"\"\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# Step 7: Test Word Embeddings\n",
    "print(\"\\nWord Embedding Similarity Results:\")\n",
    "print(f\"Similarity between 'machine' and 'python': {cosine_similarity(get_embedding('machine'), get_embedding('learning')):.4f}\")\n",
    "print(f\"Similarity between 'travel' and 'learning': {cosine_similarity(get_embedding('travel'), get_embedding('python')):.4f}\")\n",
    "print(f\"Word embedding for 'data': {get_embedding('data')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.94042464 0.98171998 0.98069883 0.97695953 0.97645927\n",
      "  0.89392703 0.91509761 0.9749875  0.98571634 0.97246832 0.91492167\n",
      "  0.98295956 0.95731231 0.98929305 0.95341077]\n",
      " [0.94042464 1.         0.97673518 0.97795519 0.97303567 0.97622313\n",
      "  0.98055748 0.99126807 0.97989985 0.96992354 0.95109866 0.9924161\n",
      "  0.97054779 0.98850313 0.96834845 0.94502817]\n",
      " [0.98171998 0.97673518 1.         0.99365536 0.99374614 0.99355454\n",
      "  0.94165167 0.9656844  0.99317391 0.99175263 0.9817941  0.95735299\n",
      "  0.99124326 0.98683114 0.99696885 0.96209684]\n",
      " [0.98069883 0.97795519 0.99365536 1.         0.99828126 0.99807437\n",
      "  0.93676608 0.96174042 0.99724244 0.99874707 0.97417839 0.95787639\n",
      "  0.99786251 0.99010397 0.99171664 0.97867969]\n",
      " [0.97695953 0.97303567 0.99374614 0.99828126 1.         0.9989772\n",
      "  0.93462841 0.96069924 0.99830544 0.99757484 0.97339091 0.95590927\n",
      "  0.99816774 0.98855815 0.99219863 0.97769764]\n",
      " [0.97645927 0.97622313 0.99355454 0.99807437 0.9989772  1.\n",
      "  0.94131453 0.96483019 0.9990153  0.99688321 0.96657578 0.95872098\n",
      "  0.99764461 0.99238173 0.99145859 0.97379815]\n",
      " [0.89392703 0.98055748 0.94165167 0.93676608 0.93462841 0.94131453\n",
      "  1.         0.9878813  0.94739935 0.92926736 0.89810348 0.99035557\n",
      "  0.93465442 0.95859298 0.93403465 0.87959855]\n",
      " [0.91509761 0.99126807 0.9656844  0.96174042 0.96069924 0.96483019\n",
      "  0.9878813  1.         0.96820762 0.9533824  0.92587097 0.99052429\n",
      "  0.95677583 0.98264259 0.95336472 0.92472823]\n",
      " [0.9749875  0.97989985 0.99317391 0.99724244 0.99830544 0.9990153\n",
      "  0.94739935 0.96820762 1.         0.99542947 0.9690011  0.96512664\n",
      "  0.9965776  0.99298893 0.99083276 0.97451078]\n",
      " [0.98571634 0.96992354 0.99175263 0.99874707 0.99757484 0.99688321\n",
      "  0.92926736 0.9533824  0.99542947 1.         0.97325603 0.95070249\n",
      "  0.99940307 0.98456293 0.99290581 0.97737223]\n",
      " [0.97246832 0.95109866 0.9817941  0.97417839 0.97339091 0.96657578\n",
      "  0.89810348 0.92587097 0.9690011  0.97325603 1.         0.93055111\n",
      "  0.97007245 0.95274055 0.98209908 0.95851419]\n",
      " [0.91492167 0.9924161  0.95735299 0.95787639 0.95590927 0.95872098\n",
      "  0.99035557 0.99052429 0.96512664 0.95070249 0.93055111 1.\n",
      "  0.95485393 0.97214846 0.95188096 0.92119568]\n",
      " [0.98295956 0.97054779 0.99124326 0.99786251 0.99816774 0.99764461\n",
      "  0.93465442 0.95677583 0.9965776  0.99940307 0.97007245 0.95485393\n",
      "  1.         0.98514514 0.99299524 0.97502794]\n",
      " [0.95731231 0.98850313 0.98683114 0.99010397 0.98855815 0.99238173\n",
      "  0.95859298 0.98264259 0.99298893 0.98456293 0.95274055 0.97214846\n",
      "  0.98514514 1.         0.97787654 0.96914575]\n",
      " [0.98929305 0.96834845 0.99696885 0.99171664 0.99219863 0.99145859\n",
      "  0.93403465 0.95336472 0.99083276 0.99290581 0.98209908 0.95188096\n",
      "  0.99299524 0.97787654 1.         0.95728729]\n",
      " [0.95341077 0.94502817 0.96209684 0.97867969 0.97769764 0.97379815\n",
      "  0.87959855 0.92472823 0.97451078 0.97737223 0.95851419 0.92119568\n",
      "  0.97502794 0.96914575 0.95728729 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "cosine_sim_matrix = lambda embeddings: (embeddings @ embeddings.T) / (np.linalg.norm(embeddings, axis=1, keepdims=True) @ np.linalg.norm(embeddings, axis=1, keepdims=True).T)\n",
    "cos_sim_matrix = cosine_sim_matrix(model.get_word_vectors())  # Compute cosine similarity matrix\n",
    "print(cos_sim_matrix)  # Shape: (vocab_size, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BatchSkipGram:\n",
    "    def __init__(self, vocab_size, embedding_dim, learning_rate=0.05, batch_size=4):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Initialize U (context matrix) and V (center matrix) randomly\n",
    "        self.U = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
    "        self.V = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"Compute softmax row-wise with numerical stability.\"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Stability trick\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    def compute_loss_and_gradients(self, batch):\n",
    "        \"\"\"\n",
    "        Compute loss and gradients for a batch of (center, context) pairs.\n",
    "        batch: List of (center_word_index, context_word_index) pairs.\n",
    "        \"\"\"\n",
    "        batch_size = len(batch)\n",
    "        centers, contexts = zip(*batch)  # Extract word indices\n",
    "        centers = np.array(centers)\n",
    "        contexts = np.array(contexts)\n",
    "\n",
    "        # Compute scores (dot product between context and all center words)\n",
    "        scores = np.dot(self.U[contexts], self.V.T)  # Shape: (batch_size, vocab_size)\n",
    "\n",
    "        # Compute softmax probabilities\n",
    "        P = self.softmax(scores)  # Shape: (batch_size, vocab_size)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = -np.sum(np.log(P[np.arange(batch_size), centers])) / batch_size\n",
    "\n",
    "        # Compute gradients\n",
    "        dU = np.zeros_like(self.U)\n",
    "        dV = np.zeros_like(self.V)\n",
    "\n",
    "        # One-hot encode centers for batch (for efficient subtraction)\n",
    "        center_one_hot = np.zeros((batch_size, self.vocab_size))\n",
    "        center_one_hot[np.arange(batch_size), centers] = 1\n",
    "\n",
    "        # Compute difference between actual and predicted probabilities\n",
    "        diff = P - center_one_hot  # Shape: (batch_size, vocab_size)\n",
    "\n",
    "        # Update context word gradients\n",
    "        dU[contexts] = np.dot(diff, self.V)  # Shape correction\n",
    "\n",
    "        # Update center word gradients\n",
    "        dV[centers] -= self.U[contexts]  # Pull correct pairs together\n",
    "        for i in range(batch_size):  # Accumulate over batch\n",
    "            dV += P[i, :, np.newaxis] * self.U[contexts[i]]\n",
    "\n",
    "        # Normalize gradients by batch size\n",
    "        dU /= batch_size\n",
    "        dV /= batch_size\n",
    "\n",
    "        return loss, dU, dV\n",
    "\n",
    "    def train(self, dataset, epochs=1000):\n",
    "        \"\"\"\n",
    "        Train model using batch gradient descent.\n",
    "        dataset: List of (center_word_index, context_word_index) pairs.\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            np.random.shuffle(dataset)  # Shuffle dataset each epoch\n",
    "            total_loss = 0\n",
    "\n",
    "            for i in range(0, len(dataset), self.batch_size):\n",
    "                batch = dataset[i:i+self.batch_size]  # Select batch\n",
    "                loss, dU, dV = self.compute_loss_and_gradients(batch)\n",
    "\n",
    "                # Update matrices\n",
    "                self.U -= self.learning_rate * dU\n",
    "                self.V -= self.learning_rate * dV\n",
    "\n",
    "                total_loss += loss\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "    def get_word_vectors(self):\n",
    "        \"\"\"Return learned word embeddings (average of U and V).\"\"\"\n",
    "        return (self.U + self.V) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 97.0407\n",
      "Epoch 100, Loss: 57.8623\n",
      "Epoch 200, Loss: 57.9518\n",
      "Epoch 300, Loss: 58.0282\n",
      "Epoch 400, Loss: 58.1466\n",
      "Final Word Embeddings:\n",
      " [[ 2.39144987e-01 -7.01049044e-01  2.68894793e-01  1.10664816e+00\n",
      "  -2.25121295e-01 -6.11209245e-01 -6.80401612e-01  5.53725523e-01\n",
      "   3.78910217e-01  1.29782901e+00]\n",
      " [ 1.04369246e+00 -7.62884561e-01 -1.17013089e+00  1.50808783e+00\n",
      "   1.70200350e+00  7.36227092e-03 -3.37419575e-01 -1.90422540e-01\n",
      "  -1.04063607e+00  7.46799017e-01]\n",
      " [ 8.45473856e-01  3.37794200e-01  3.99465129e-01  1.05383220e+00\n",
      "  -1.13618163e-01 -2.45329733e-01  3.05108322e-01  2.12893817e-01\n",
      "   4.13836991e-01  8.77235229e-01]\n",
      " [ 5.75995829e-01  3.26653364e-01 -1.80005394e-01  3.52405767e-01\n",
      "  -1.63688492e-01  4.90782250e-01  2.36014439e-01 -2.02098546e-01\n",
      "   4.50332346e-01  1.45290984e-01]\n",
      " [ 4.95682529e-01  6.09648552e-01 -1.36815463e+00  2.25835374e-01\n",
      "   1.35363882e-01  1.11608133e+00 -1.26510591e-01 -6.64199688e-02\n",
      "   4.33796937e-01  9.06313988e-02]\n",
      " [-3.35533252e-03 -5.82580089e-02 -2.37497155e-01  6.25008601e-02\n",
      "   1.67342370e-01  9.57797486e-02  3.83029615e-01  1.34811931e-01\n",
      "  -1.66798549e-01 -6.81917884e-04]\n",
      " [ 1.44964944e+00 -7.36199432e-01 -1.08816212e+00  2.03323457e+00\n",
      "   1.44676407e+00  1.62912757e-01 -8.56906994e-01 -6.58156062e-02\n",
      "  -6.83217296e-01  8.90717370e-01]\n",
      " [ 8.85440273e-01 -7.32894924e-01 -1.15508198e+00  1.28450871e+00\n",
      "   1.69699730e+00 -9.47478317e-02 -1.37893882e-01 -1.76635218e-01\n",
      "  -1.19176196e+00  3.63347169e-01]\n",
      " [ 1.24218164e-02  3.12446801e-02 -1.00628772e+00 -4.27649700e-02\n",
      "   4.59289580e-01 -1.93491857e-01 -8.98325358e-02  2.36263161e-01\n",
      "   1.20515130e-01  1.41374166e-01]\n",
      " [-3.30818990e-01 -3.68342647e-01 -4.12644930e-01 -8.00691352e-02\n",
      "   1.86667061e-01 -8.00659367e-01 -8.51446557e-01  2.69146428e-01\n",
      "   1.50189897e-01  3.85687929e-01]\n",
      " [ 9.87371304e-01  5.75792659e-01 -1.43297261e+00  6.76094254e-01\n",
      "  -2.52483139e-01  1.08642937e+00  5.21130784e-01 -9.67279858e-03\n",
      "   9.73165017e-01  5.60942366e-01]\n",
      " [ 1.44526889e+00 -7.32985460e-01 -1.04834377e+00  1.56690124e+00\n",
      "   1.64278663e+00 -2.27284486e-02 -9.11122186e-01 -3.01917304e-01\n",
      "  -7.37830239e-01  6.62068521e-01]\n",
      " [-4.09323557e-01 -5.43195359e-01 -1.51815675e-02  4.62107659e-01\n",
      "   1.02519588e-01 -4.56866328e-01 -1.04789237e+00  6.08002620e-01\n",
      "   1.94854647e-01  8.51580335e-01]\n",
      " [ 4.11572418e-01 -4.34105944e-01 -6.59359478e-01  2.20415073e-01\n",
      "   1.37668142e+00 -2.78430378e-01 -5.38371324e-02 -4.21220028e-01\n",
      "  -8.94549942e-01 -3.04561880e-01]\n",
      " [ 6.57064509e-01  4.64989139e-02  2.14571556e-01  1.28884820e+00\n",
      "  -5.48471737e-01 -2.34745692e-01 -1.47986942e-01  3.73216353e-01\n",
      "   2.99285516e-01  1.06265911e+00]\n",
      " [ 1.68645413e-01 -2.50727609e-01 -1.58025134e+00 -4.62573882e-01\n",
      "   3.80156998e-01  9.67590509e-01 -4.97766066e-01 -3.69527421e-01\n",
      "   4.19355457e-01 -2.39100619e-01]]\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 10\n",
    "batch_size = 2  # Process 2 word pairs at a time\n",
    "\n",
    "# Initialize and train model\n",
    "model = BatchSkipGram(vocab_size, embedding_dim, learning_rate=0.1, batch_size=batch_size)\n",
    "model.train(training_data, epochs=500)\n",
    "\n",
    "# Get trained word embeddings\n",
    "word_vectors = model.get_word_vectors()\n",
    "print(\"Final Word Embeddings:\\n\", word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Embedding Similarity Results:\n",
      "Similarity between 'machine' and 'python': 0.8275\n",
      "Similarity between 'travel' and 'learning': 0.2994\n",
      "Word embedding for 'data': [ 0.84547386  0.3377942   0.39946513  1.0538322  -0.11361816 -0.24532973\n",
      "  0.30510832  0.21289382  0.41383699  0.87723523]\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(word):\n",
    "    \"\"\"\n",
    "    Retrieves the learned embedding for a given word.\n",
    "    \"\"\"\n",
    "    return word_vectors[word2idx[word]]\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two word vectors.\n",
    "    \"\"\"\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# Step 7: Test Word Embeddings\n",
    "print(\"\\nWord Embedding Similarity Results:\")\n",
    "print(f\"Similarity between 'machine' and 'python': {cosine_similarity(get_embedding('machine'), get_embedding('learning')):.4f}\")\n",
    "print(f\"Similarity between 'travel' and 'learning': {cosine_similarity(get_embedding('travel'), get_embedding('python')):.4f}\")\n",
    "print(f\"Word embedding for 'data': {get_embedding('data')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
